{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Three**\n",
    "\n",
    "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Understand N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>3-grams</th>\n",
       "      <th>Count of 3-grams</th>\n",
       "      <th>Probability of bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every once in a while a movie comes, that trul...</td>\n",
       "      <td>[everi, come, truli, make, impact, joaquin, pe...</td>\n",
       "      <td>[(everi, come, truli), (come, truli, make), (t...</td>\n",
       "      <td>52</td>\n",
       "      <td>[((everi, come), 1.0), ((come, come), 1.0), ((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a movie that only those who have felt ...</td>\n",
       "      <td>[thi, felt, alon, isol, truli, relat, you, und...</td>\n",
       "      <td>[(thi, felt, alon), (felt, alon, isol), (alon,...</td>\n",
       "      <td>46</td>\n",
       "      <td>[((thi, felt), 1.0), ((felt, felt), 1.0), ((fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
       "      <td>[truli, masterpiec, the, best, hollywood, film...</td>\n",
       "      <td>[(truli, masterpiec, the), (masterpiec, the, b...</td>\n",
       "      <td>91</td>\n",
       "      <td>[((truli, masterpiec), 0.5), ((masterpiec, mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
       "      <td>[joaquin, phoenix, give, tour, de, forc, perfo...</td>\n",
       "      <td>[(joaquin, phoenix, give), (phoenix, give, tou...</td>\n",
       "      <td>58</td>\n",
       "      <td>[((joaquin, phoenix), 1.0), ((phoenix, phoenix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most of the time movies are anticipated like t...</td>\n",
       "      <td>[most, time, movi, anticip, like, end, fall, s...</td>\n",
       "      <td>[(most, time, movi), (time, movi, anticip), (m...</td>\n",
       "      <td>58</td>\n",
       "      <td>[((most, time), 1.0), ((time, time), 0.5), ((t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Every once in a while a movie comes, that trul...   \n",
       "1  This is a movie that only those who have felt ...   \n",
       "2  Truly a masterpiece, The Best Hollywood film o...   \n",
       "3  Joaquin Phoenix gives a tour de force performa...   \n",
       "4  Most of the time movies are anticipated like t...   \n",
       "\n",
       "                                      cleaned_review  \\\n",
       "0  [everi, come, truli, make, impact, joaquin, pe...   \n",
       "1  [thi, felt, alon, isol, truli, relat, you, und...   \n",
       "2  [truli, masterpiec, the, best, hollywood, film...   \n",
       "3  [joaquin, phoenix, give, tour, de, forc, perfo...   \n",
       "4  [most, time, movi, anticip, like, end, fall, s...   \n",
       "\n",
       "                                             3-grams  Count of 3-grams  \\\n",
       "0  [(everi, come, truli), (come, truli, make), (t...                52   \n",
       "1  [(thi, felt, alon), (felt, alon, isol), (alon,...                46   \n",
       "2  [(truli, masterpiec, the), (masterpiec, the, b...                91   \n",
       "3  [(joaquin, phoenix, give), (phoenix, give, tou...                58   \n",
       "4  [(most, time, movi), (time, movi, anticip), (m...                58   \n",
       "\n",
       "                              Probability of bigrams  \n",
       "0  [((everi, come), 1.0), ((come, come), 1.0), ((...  \n",
       "1  [((thi, felt), 1.0), ((felt, felt), 1.0), ((fe...  \n",
       "2  [((truli, masterpiec), 0.5), ((masterpiec, mas...  \n",
       "3  [((joaquin, phoenix), 1.0), ((phoenix, phoenix...  \n",
       "4  [((most, time), 1.0), ((time, time), 0.5), ((t...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Getting data from Assignment 02.\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"C:\\Users\\Raheyma Arshad\\Desktop\\Output_CSV.csv\")\n",
    "data = data[['review', 'cleaned_review']]\n",
    "\n",
    "# Tokenization of cleaned reviews.\n",
    "import nltk\n",
    "data['cleaned_review'] = data['cleaned_review'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# (1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "from nltk.util import ngrams\n",
    "data['3-grams'] = data['cleaned_review'].apply(lambda row: list(nltk.ngrams(row, 3)))\n",
    "data['Count of 3-grams'] = data['3-grams'].str.len()\n",
    "\n",
    "# (2) Calculate the probabilities for all the bigrams in the dataset.\n",
    "\n",
    "def probability(x):\n",
    "    probabilities = []\n",
    "    bigrams = list(nltk.ngrams(x, 2))\n",
    "    for (w1, w2) in bigrams:\n",
    "        numerator = bigrams.count((w1, w2))\n",
    "        for w1 in (w1, w2):\n",
    "            denominator = x.count(w1)\n",
    "            probabilities.append(((w1, w2), numerator/denominator))\n",
    "    return probabilities\n",
    "\n",
    "data['Probability of bigrams'] = data['cleaned_review'].apply(lambda x: probability(x))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Raheyma\\Software\\Anaconda\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max frequency (noun phrase) is: ('it', 348)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Noun Phrases</th>\n",
       "      <th>Relative Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every once in a while a movie comes, that trul...</td>\n",
       "      <td>[a while, a movie, an impact, joaquin's perfor...</td>\n",
       "      <td>[(a while, 0.00287), (a movie, 0.00287), (an i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a movie that only those who have felt ...</td>\n",
       "      <td>[a movie, who, it, you, the motive, you, the c...</td>\n",
       "      <td>[(a movie, 0.00287), (who, 0.00287), (it, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
       "      <td>[truly a masterpiece, the best hollywood film,...</td>\n",
       "      <td>[(truly a masterpiece, 0.00287), (the best hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
       "      <td>[joaquin phoenix, a tour de force performance,...</td>\n",
       "      <td>[(joaquin phoenix, 0.00287), (a tour de force ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most of the time movies are anticipated like t...</td>\n",
       "      <td>[the time, movies, they, joker, the first time...</td>\n",
       "      <td>[(the time, 0.00287), (movies, 0.00287), (they...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  \\\n",
       "0  Every once in a while a movie comes, that trul...   \n",
       "1  This is a movie that only those who have felt ...   \n",
       "2  Truly a masterpiece, The Best Hollywood film o...   \n",
       "3  Joaquin Phoenix gives a tour de force performa...   \n",
       "4  Most of the time movies are anticipated like t...   \n",
       "\n",
       "                                        Noun Phrases  \\\n",
       "0  [a while, a movie, an impact, joaquin's perfor...   \n",
       "1  [a movie, who, it, you, the motive, you, the c...   \n",
       "2  [truly a masterpiece, the best hollywood film,...   \n",
       "3  [joaquin phoenix, a tour de force performance,...   \n",
       "4  [the time, movies, they, joker, the first time...   \n",
       "\n",
       "                              Relative Probabilities  \n",
       "0  [(a while, 0.00287), (a movie, 0.00287), (an i...  \n",
       "1  [(a movie, 0.00287), (who, 0.00287), (it, 0.00...  \n",
       "2  [(truly a masterpiece, 0.00287), (the best hol...  \n",
       "3  [(joaquin phoenix, 0.00287), (a tour de force ...  \n",
       "4  [(the time, 0.00287), (movies, 0.00287), (they...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews.\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def noun_phrases(x):\n",
    "    x = x.replace('[^\\w\\s]','')\n",
    "    x = x.lower()\n",
    "    nps = []\n",
    "    doc = nlp(x)\n",
    "    chunks = list(doc.noun_chunks)\n",
    "    for chunk in chunks:\n",
    "        nps.append(chunk)\n",
    "    return nps\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Reviews'] = data['review']\n",
    "df['Noun Phrases'] = df['Reviews'].apply(lambda x: noun_phrases(x))\n",
    "df['Noun Phrases'] = df['Noun Phrases'].apply(lambda x: [str(item) for item in x])\n",
    "np_list = df[\"Noun Phrases\"].tolist()\n",
    "np_flat_list = [str(item) for sublist in np_list for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "np_freq = Counter(np_flat_list)\n",
    "print('max frequency (noun phrase) is:', np_freq.most_common(1)[0])\n",
    "max_freq = np_freq.most_common(1)[0][1]\n",
    "\n",
    "def relative_prob(x):\n",
    "    phrase_prob = []\n",
    "    phrases = []\n",
    "    for phrase in x:\n",
    "        if phrase not in phrases:\n",
    "            freq = x.count(phrase)\n",
    "            rel_prob = round(freq/max_freq, 5)\n",
    "            phrase_prob.append((phrase, rel_prob))\n",
    "            phrases.append(phrase)\n",
    "    return phrase_prob\n",
    "\n",
    "df['Relative Probabilities'] = df['Noun Phrases'].apply(lambda x: relative_prob(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Undersand TF-IDF and Document representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
    "\n",
    "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aaah</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absurd</th>\n",
       "      <th>abus</th>\n",
       "      <th>accept</th>\n",
       "      <th>acclaim</th>\n",
       "      <th>accolad</th>\n",
       "      <th>accompli</th>\n",
       "      <th>...</th>\n",
       "      <th>you</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>youv</th>\n",
       "      <th>zazi</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimmer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rev1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev2</th>\n",
       "      <td>0.039173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev3</th>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  a  aaah  abl  absolut  absurd  abus  accept  acclaim  \\\n",
       "review_id                                                                \n",
       "rev1       0.000000   0.0  0.0      0.0     0.0   0.0     0.0      0.0   \n",
       "rev2       0.039173   0.0  0.0      0.0     0.0   0.0     0.0      0.0   \n",
       "rev3       0.020218   0.0  0.0      0.0     0.0   0.0     0.0      0.0   \n",
       "rev4       0.000000   0.0  0.0      0.0     0.0   0.0     0.0      0.0   \n",
       "rev5       0.000000   0.0  0.0      0.0     0.0   0.0     0.0      0.0   \n",
       "\n",
       "           accolad  accompli  ...       you  youd  youll  young  younger  \\\n",
       "review_id                     ...                                          \n",
       "rev1           0.0       0.0  ...  0.000000   0.0    0.0    0.0      0.0   \n",
       "rev2           0.0       0.0  ...  0.051961   0.0    0.0    0.0      0.0   \n",
       "rev3           0.0       0.0  ...  0.000000   0.0    0.0    0.0      0.0   \n",
       "rev4           0.0       0.0  ...  0.000000   0.0    0.0    0.0      0.0   \n",
       "rev5           0.0       0.0  ...  0.000000   0.0    0.0    0.0      0.0   \n",
       "\n",
       "           your  youv  zazi  zero  zimmer  \n",
       "review_id                                  \n",
       "rev1        0.0   0.0   0.0   0.0     0.0  \n",
       "rev2        0.0   0.0   0.0   0.0     0.0  \n",
       "rev3        0.0   0.0   0.0   0.0     0.0  \n",
       "rev4        0.0   0.0   0.0   0.0     0.0  \n",
       "rev5        0.0   0.0   0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 2181 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Building the documents-terms weights (tf*idf) matrix\n",
    "\n",
    "# Creating a corpus of reviews.\n",
    "import pandas as pd\n",
    "reviews = pd.read_csv(r\"C:\\Users\\Raheyma Arshad\\Desktop\\Output_CSV - Copy.csv\")\n",
    "corpus = reviews.set_index('review_id').T.to_dict('list')\n",
    "\n",
    "def listToString(s):      \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "\n",
    "corpus = {doc_label: listToString(s) for doc_label, s in corpus.items()}\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Tokenize whole corpus\n",
    "tokens = {doc_label: nltk.word_tokenize(text) for doc_label, text in corpus.items()}\n",
    "\n",
    "# Count the tokens for each document\n",
    "from collections import Counter\n",
    "counts = {doc_label: Counter(tok) for doc_label, tok in tokens.items()}\n",
    "\n",
    "# Extract the vocabulary (set of unique terms in all documents)\n",
    "vocab = set()\n",
    "for counter in counts.values():\n",
    "    vocab |= set(counter.keys())    \n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "# Create Bag of Words matrix: rows are documents, columns are vocabulary words.\n",
    "bow = []\n",
    "for counter in counts.values():\n",
    "    bow_row = [counter.get(term, 0) for term in vocab]\n",
    "    bow.append(bow_row)\n",
    "doc_labels = list(counts.keys())\n",
    "\n",
    "# Term Frequency\n",
    "import numpy as np\n",
    "raw_counts = np.mat(bow, dtype=float)       \n",
    "tf = raw_counts / np.sum(raw_counts, axis=1)\n",
    "\n",
    "# idf – inverse document frequency\n",
    "def num_term_in_docs(t, docs):\n",
    "    return sum(t in d for d in docs.values())\n",
    "\n",
    "from math import log\n",
    "def idf(t, docs):\n",
    "    a = num_term_in_docs(t, docs)\n",
    "    return log(1 + len(docs) / (1+a))\n",
    "\n",
    "idf_row = [idf(t, tokens) for t in vocab]\n",
    "idf_mat = np.mat(np.diag(idf_row))\n",
    "tfidf = tf * idf_mat\n",
    "\n",
    "# Presenting the tf-idf matrix as a dataframe\n",
    "tfidf_arrays = np.array(tfidf)\n",
    "tfidf_df = pd.DataFrame(data=tfidf_arrays, index=reviews['review_id'], columns=vocab)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>Cosine Similarity with Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rev1</td>\n",
       "      <td>0.028861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rev2</td>\n",
       "      <td>0.004285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rev3</td>\n",
       "      <td>0.005668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rev4</td>\n",
       "      <td>0.006743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rev5</td>\n",
       "      <td>0.003087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id  Cosine Similarity with Query\n",
       "0      rev1                      0.028861\n",
       "1      rev2                      0.004285\n",
       "2      rev3                      0.005668\n",
       "3      rev4                      0.006743\n",
       "4      rev5                      0.003087"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) To rank the documents with respect to query by using cosine similarity.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "\n",
    "def clean_query(query):\n",
    "    query = query.replace('[^\\w\\s]','')\n",
    "    query = query.replace('\\d+', '')\n",
    "    query = \" \".join(x for x in query.split() if x not in stop)\n",
    "    query = \" \".join(x.lower() for x in query.split())\n",
    "    query = \" \".join([PorterStemmer().stem(word) for word in query.split()])\n",
    "    query = \" \".join([Word(word).lemmatize() for word in query.split()])\n",
    "    return query\n",
    "\n",
    "def tfidf_vector_of_query(query):\n",
    "    q_corpus = {'query': clean_query(query)}\n",
    "    \n",
    "    q_tokens = {doc_label: nltk.word_tokenize(text) for doc_label, text in q_corpus.items()}\n",
    "    q_counts = {doc_label: Counter(tok) for doc_label, tok in q_tokens.items()}\n",
    "\n",
    "    q_bow = []\n",
    "    for counter in q_counts.values():\n",
    "        q_bow_row = [counter.get(term, 0) for term in vocab]\n",
    "        q_bow.append(q_bow_row)\n",
    "    q_label = list(q_counts.keys())\n",
    "\n",
    "    q_raw_counts = np.mat(q_bow, dtype=float)       \n",
    "    q_tf = q_raw_counts / np.sum(q_raw_counts, axis=1)\n",
    "\n",
    "    q_idf_row = [idf(t, q_tokens) for t in vocab]\n",
    "    q_idf_mat = np.mat(np.diag(q_idf_row))\n",
    "    q_tfidf = q_tf * q_idf_mat\n",
    "\n",
    "    q_tfidf_array = np.array(q_tfidf)\n",
    "    return q_tfidf_array\n",
    "\n",
    "from numpy import linalg as la\n",
    "\n",
    "def cosine_similarity(query, rev):\n",
    "    rev = np.array(rev)\n",
    "    query = tfidf_vector_of_query(query)\n",
    "    if la.norm(query) != 0 and la.norm(rev) != 0:\n",
    "        cos_sim = np.inner(query, rev) / la.norm(query) * la.norm(rev)\n",
    "    else:\n",
    "        cos_sim = 0\n",
    "    return float(cos_sim)\n",
    "\n",
    "query = \"An Outstanding movie with a haunting performance and best character development\"\n",
    "\n",
    "cos_sim_list = []\n",
    "for i in range(len(tfidf_df)):\n",
    "    cos_sim = cosine_similarity(query, tfidf_df.iloc[i])\n",
    "    cos_sim_list.append(cos_sim)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'review_id': doc_labels, 'Cosine Similarity with Query': cos_sim_list})\n",
    "cos_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# The GitHub link of your final csv file\n",
    "\n",
    "# Link: https://github.com/raheymakhan/Assignment-03/blob/main/Annotated%20Data.csv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMniIalS+f3MyeuLTJeFDvi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
